{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "FHGGU1kAeRM1",
      "metadata": {
        "id": "FHGGU1kAeRM1"
      },
      "source": [
        "# **Countering Radicalization & Algorithmic Discrimination in Recommender Systems**\n",
        "\n",
        "**Name:** Umais Siddiqui\n",
        "\n",
        "**Class:** Data 612 - Recommender Systems\n",
        "\n",
        "**Github Link:** https://github.com/umais/DATA612_Recommender_Systems/blob/master/Week4/recommender_radicalization_reflection.ipynb\n",
        "\n",
        "\n",
        "##  **Reflection Based on Key Articles**\n",
        "\n",
        "This notebook summarizes and reflects on the radicalizing effects of recommender systems and proposes ways to prevent algorithmic discrimination. The discussion is based on the following articles:\n",
        "\n",
        "- **Renee DiResta (2018)**: *Up Next: A Better Recommendation System*, Wired\n",
        "- **Zeynep Tufekci (2018)**: *YouTube, the Great Radicalizer*, New York Times\n",
        "- **Sanjay Krishnan et al.**: *Social Influence Bias in Recommender Systems*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fOaEoP_CeRM5",
      "metadata": {
        "id": "fOaEoP_CeRM5"
      },
      "source": [
        "##  **The Problem**\n",
        "\n",
        "Recommender systems, particularly on platforms like YouTube, TikTok, and Facebook, use engagement-driven algorithms that prioritize content likely to retain attention. While effective in driving user interaction, this design has an unintended consequence: **pushing users toward increasingly extreme or polarizing content**, a phenomenon described as **algorithmic radicalization**.\n",
        "\n",
        "Moreover, these systems may reproduce or amplify societal biases, leading to **algorithmic discrimination** in customer segmentation, product recommendations, and content visibility.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lTYwCN8heRM5",
      "metadata": {
        "id": "lTYwCN8heRM5"
      },
      "source": [
        "## Key Takeaways from Readings\n",
        "\n",
        "**1. Renee DiResta - Wired (2018):**\n",
        "- Engagement-optimized algorithms can unintentionally promote misinformation or sensationalism.\n",
        "- Recommender systems should **incorporate friction** (e.g., highlighting credible sources, limiting autoplay) and **allow editorial oversight**.\n",
        "\n",
        "**2. Zeynep Tufekci - NYT (2018):**\n",
        "- YouTube's algorithm tends to show **increasingly extreme content** to retain user attention.\n",
        "- Suggests the need for **algorithmic transparency** and limiting optimization solely for engagement.\n",
        "\n",
        "**3. Krishnan et al. - Social Influence Bias:**\n",
        "- **Early positive or negative reviews** skew perception, leading to biased future ratings.\n",
        "- Proposes a **methodology to learn and mitigate bias** via controlled experiments and debiasing filters before training algorithms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fHoMKNiEeRM6",
      "metadata": {
        "id": "fHoMKNiEeRM6"
      },
      "source": [
        "## Countermeasures & Solutions\n",
        "\n",
        "**1. Algorithmic Transparency:**\n",
        "- Explain recommendation logic to users.\n",
        "- Provide opt-out mechanisms or profile reset tools.\n",
        "\n",
        "**2. Friction & Editorial Oversight:**\n",
        "- Slow down autoplay.\n",
        "- Elevate vetted, credible sourcesâ€”especially on controversial topics.\n",
        "\n",
        "**3. Debiasing Techniques:**\n",
        "- Apply **Bayesian smoothing** or reweighting to counter early-rating bias.\n",
        "- Design algorithms to resist influence cascades.\n",
        "\n",
        "**4. Multi-Objective Optimization:**\n",
        "- Shift from pure engagement metrics to include fairness, credibility, and diversity.\n",
        "\n",
        "**5. Auditing and Testing:**\n",
        "- Conduct fairness audits across demographic slices.\n",
        "- Simulate recommendations for edge cases and vulnerable groups.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bWVrE_LKeRM6",
      "metadata": {
        "id": "bWVrE_LKeRM6"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Unchecked recommendation systems risk amplifying radical content and reinforcing social biases. The solution lies not in discarding recommendation technology, but in improving it with **fairness aware design**, **ethical guardrails**, and **greater user control**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IztkQMvLeRM6",
      "metadata": {
        "id": "IztkQMvLeRM6"
      },
      "source": [
        "## References\n",
        "- DiResta, R. (2018). *Up Next: A Better Recommendation System*. Wired. [https://www.wired.com/story/you-tube-algorithm-change-extremism/](https://www.wired.com/story/you-tube-algorithm-change-extremism/)\n",
        "- Tufekci, Z. (2018). *YouTube, the Great Radicalizer*. NY Times. [https://www.nytimes.com/2018/03/10/opinion/sunday/youtube-politics-radical.html](https://www.nytimes.com/2018/03/10/opinion/sunday/youtube-politics-radical.html)\n",
        "- Krishnan, S., Patel, J., Franklin, M. J., & Goldberg, K. (n.d.). *Social Influence Bias in Recommender Systems*. [https://arxiv.org/abs/1809.04506](https://arxiv.org/abs/1809.04506)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
