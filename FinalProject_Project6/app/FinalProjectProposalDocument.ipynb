{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed59ade7",
   "metadata": {},
   "source": [
    "# Final Project Proposal & Project 6\n",
    "\n",
    "**Name:** Umais Siddiqui  \n",
    "**Class:** Data 612 - Recommender Systems  \n",
    "**Github Link:** https://github.com/umais/DATA612_Recommender_Systems/blob/master/FinalProject_Project6/app/FinalProjectProposalDocument.ipynb\n",
    "\n",
    "For the Final Project and also Project 6 I will be submitting as a single project. I will be fulfilling all the requirements for Project 6 in the Final Project therefore I will submit them as one. The idea is to build a personalized recommender system using Amazon’s Electronics review dataset. It will apply Spark-based collaborative filtering (ALS) to generate product recommendations for users and deploys a real-time API using Flask and Azure Kubernetes Service (AKS). All data and results are stored in Azure Blob and SQL Database.\n",
    "\n",
    "**Environment Setup**:  \n",
    "The system runs inside a Docker container hosted on an Azure Virtual Machine. Azure File Share is mounted on the VM to persist intermediate files and logs. Azure Blob is used for scalable object storage, and Azure SQL is used to store structured results.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508d223e",
   "metadata": {},
   "source": [
    "##  **Dataset Overview**\n",
    "\n",
    "- **Source**: Amazon Reviews (Electronics)\n",
    "- **Size**: ~7 million reviews\n",
    "- **Columns Used**: `reviewerID`, `asin`, `overall (rating)`, `reviewText`\n",
    "- **Goal**: Generate Top-N recommendations per user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b61d5c",
   "metadata": {},
   "source": [
    "# **Project Description and Deliverable**\n",
    "\n",
    "\n",
    "The goal of this project is to build a scalable and efficient recommender system using Apache Spark’s Alternating Least Squares (ALS) algorithm, hosted on Microsoft Azure infrastructure. The project workflow involves the following.\n",
    "\n",
    "- creating a new Azure Virtual Machine (VM) that is configured to access data stored in an Azure File Share via a mounted drive.\n",
    "- Within the VM, Docker will be installed to run Spark inside a containerized environment, ensuring portability and ease of management. \n",
    "\n",
    "\n",
    "- The Spark application will load data directly from the mounted Azure File Share, enabling seamless interaction with persistent cloud storage.\n",
    "\n",
    "\n",
    "- The recommender system will be developed by applying and iteratively improving the ALS model on the dataset to enhance its prediction accuracy and overall performance. Once the model achieves satisfactory results, it will be saved for production use.\n",
    "\n",
    "\n",
    "- To expose the recommendations for real-time usage, an API service will be created, providing endpoints for retrieving personalized product recommendations. \n",
    "\n",
    "\n",
    "- A user-friendly frontend web application will be built using Flask, serving as a visualization layer to display recommendations interactively to end-users.\n",
    "\n",
    "\n",
    "- The entire system, including the API and frontend, will be deployed and hosted on Azure, leveraging the cloud platform’s scalability and reliability.\n",
    "\n",
    "## **Final Deliverable**\n",
    "\n",
    "- Jupyter Notebook on github\n",
    "- Powerpoint presentation recorded to demonstrate the working recommender system.\n",
    "\n",
    "# **Alignment with Project 6 Requirements: Hands-on with Microsoft Azure**\n",
    "\n",
    "This final recommender system project will fully incorporate the essential Azure components specified in Project 6, demonstrating practical experience with deploying a cloud solution on Microsoft Azure:\n",
    "\n",
    "**1. Persistent Storage**\n",
    "\n",
    "The project uses Azure File Share, which is part of Azure Storage, to store datasets and the trained ALS model.\n",
    "\n",
    "This provides long-term, durable, and scalable storage for both input data and model artifacts.\n",
    "\n",
    "The mounted Azure File Share is accessed both by the Azure VM running Spark inside Docker and by the Flask application hosted on Azure App Service, ensuring consistent data availability.\n",
    "\n",
    "**2. Compute Resource**\n",
    "\n",
    "The project provisions an Azure Virtual Machine that runs Docker and hosts Apache Spark inside containers for data processing, model training, and evaluation.\n",
    "\n",
    "The trained ALS model is then served through a Flask web application deployed on Azure App Service, a fully managed platform-as-a-service (PaaS) compute environment.\n",
    "\n",
    "This multi-tier compute architecture showcases the use of different Azure compute options: IaaS (VM) and PaaS (App Service).\n",
    "\n",
    "**3. Network Security**\n",
    "\n",
    "The Azure VM is placed within an Azure Virtual Network (VNet) to isolate and secure network traffic.\n",
    "\n",
    "Appropriate Network Security Groups (NSGs) are configured to restrict inbound and outbound access, ensuring only authorized users and services can connect to the VM and Azure File Share.\n",
    "\n",
    "Similarly, the Azure App Service uses private endpoints or VNet integration to securely access the Azure File Share and backend resources.\n",
    "\n",
    "This setup enforces secure communication channels between compute and storage services, complying with best practices for cloud security.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19c5da",
   "metadata": {},
   "source": [
    "# Initial Setup, Data Cleaning, Analysis, and Model Generation\n",
    "\n",
    "The project begins with setting up the environment on an Azure Virtual Machine, where Docker is installed and configured to run Apache Spark in a containerized environment. The dataset, stored on an Azure File Share and mounted inside the container, is accessed directly for processing.\n",
    "\n",
    "# Data Cleaning and Exploration\n",
    "The raw dataset is loaded into a Spark DataFrame. As can be seen below\n",
    "\n",
    "**Initial data cleaning steps are performed, including:**\n",
    "\n",
    "- Selecting relevant columns (e.g., user IDs, item IDs, and ratings).\n",
    "\n",
    "- Dropping rows with missing values to ensure data quality.\n",
    "\n",
    "- Filtering users and items to retain only those with sufficient interaction counts (e.g., users with at least 5 ratings and items rated by at least 5 users).\n",
    "\n",
    "Exploratory data analysis is conducted to understand the distribution of ratings, user behavior, and item popularity, which informs model tuning decisions.\n",
    "\n",
    "**Model Building**\n",
    "The cleaned dataset is then used to train a collaborative filtering model using Spark’s ALS (Alternating Least Squares) algorithm.\n",
    "\n",
    "The model training includes hyperparameter tuning and iterative improvements aimed at enhancing recommendation accuracy.\n",
    "\n",
    "Upon achieving satisfactory performance metrics, the model is saved to the mounted Azure File Share for persistence and later use by the flask application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a69a822a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/12 20:03:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/12 20:03:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n",
      "+--------------+----------+-------+\n",
      "|    reviewerID|      asin|overall|\n",
      "+--------------+----------+-------+\n",
      "| AO94DHGC771SJ|0528881469|    5.0|\n",
      "| AMO214LNFCEI4|0528881469|    1.0|\n",
      "|A3N7T0DY83Y4IG|0528881469|    3.0|\n",
      "|A1H8PY3QHMQQA0|0528881469|    2.0|\n",
      "|A24EV6RXELQZ63|0528881469|    1.0|\n",
      "+--------------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Initialize Spark session with proper Jetty JARs\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"AmazonRecommender\")\n",
    "    .config(\"spark.driver.extraClassPath\",\n",
    "            \"/opt/spark-3.4.1-bin-hadoop3/jars/jetty-6.1.26.jar:\" +\n",
    "            \"/opt/spark-3.4.1-bin-hadoop3/jars/jetty-util-6.1.26.jar:\" +\n",
    "            \"/opt/spark-3.4.1-bin-hadoop3/jars/jetty-ajax-6.1.26.jar\")\n",
    "    .config(\"spark.executor.extraClassPath\",\n",
    "            \"/opt/spark-3.4.1-bin-hadoop3/jars/jetty-6.1.26.jar:\" +\n",
    "            \"/opt/spark-3.4.1-bin-hadoop3/jars/jetty-util-6.1.26.jar:\" +\n",
    "            \"/opt/spark-3.4.1-bin-hadoop3/jars/jetty-ajax-6.1.26.jar\")\n",
    "    .config(\"spark.pyspark.python\", \"/usr/local/bin/python3\")\n",
    "    .config(\"spark.pyspark.driver.python\", \"/usr/local/bin/python3\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "# Read data from the mounted Azure File Share directory\n",
    "df = spark.read.json(\"/media/amazonratings/Electronics_5.json\")\n",
    "\n",
    "# Show schema and sample data\n",
    "df.printSchema()\n",
    "df.select(\"reviewerID\", \"asin\", \"overall\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18670200",
   "metadata": {},
   "source": [
    "## **Data Cleaning & Filtering**\n",
    "\n",
    "To reduce noise and improve model performance:\n",
    "- Users with fewer than 5 reviews are excluded\n",
    "- Items with fewer than 5 reviews are excluded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85cbe6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users with at least 5 items: 192403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items with at least 5 users: 63001\n",
      "User counts with >= 5 items:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|          user|user_count|\n",
      "+--------------+----------+\n",
      "|A18FTRFQQ141CP|         5|\n",
      "|A2GPNXFUUV51ZZ|         7|\n",
      "|A15K7HV1XD6YWR|         8|\n",
      "|A3PDGWYC08DXF4|        15|\n",
      "|  A44UKZE6XEV9|        16|\n",
      "|A3FE9EUVTU3UD8|         8|\n",
      "|A3DKP8M0GSP8UK|        26|\n",
      "|A37LCWTTQMBMFX|         6|\n",
      "|A141E91QV31KER|         6|\n",
      "|A1SCWY8O0IL2HU|        29|\n",
      "| AWWBZZXN32I6H|         7|\n",
      "|A1PG70NH85K859|        34|\n",
      "|A1IJOBQD8CY8K1|        25|\n",
      "|A2690TEJA2N778|        11|\n",
      "|A3GHZZM7CNK77I|         5|\n",
      "|A3TPM2VJA0X1Y2|         5|\n",
      "|A1MRESWHA86B5B|         5|\n",
      "|A2X8NZUNAWX9SO|         7|\n",
      "|A2WY7M2G4FUK9Y|         8|\n",
      "|A2JL1GIC0JAFW9|        15|\n",
      "+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Item counts with >= 5 users:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 343:==================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      item|item_count|\n",
      "+----------+----------+\n",
      "|B00000J3Q1|        14|\n",
      "|B00001W0DC|        15|\n",
      "|B00003OPEV|         6|\n",
      "|B00005853W|         5|\n",
      "|B00005Q5U5|        33|\n",
      "|B00005T3Z7|        16|\n",
      "|B000068UY7|        24|\n",
      "|B00006JLOT|        18|\n",
      "|B000083GPS|         5|\n",
      "|B00008WIX2|         6|\n",
      "|B00008ZPN3|         8|\n",
      "|B00009R6FQ|         5|\n",
      "|B0000AKACN|         9|\n",
      "|B0000E6FY7|         5|\n",
      "|B0000UV0IQ|         8|\n",
      "|B0001CLYAW|         6|\n",
      "|B00021EE4U|        67|\n",
      "|B00021Z98A|        14|\n",
      "|B0002D05RI|         5|\n",
      "|B0002D6PNQ|         8|\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "ratings_df = df.selectExpr(\"reviewerID as user\", \"asin as item\", \"overall as rating\").dropna()\n",
    "user_counts = ratings_df.groupBy(\"user\").agg(count(\"item\").alias(\"user_count\")).filter(\"user_count >= 5\")\n",
    "item_counts = ratings_df.groupBy(\"item\").agg(count(\"user\").alias(\"item_count\")).filter(\"item_count >= 5\")\n",
    "\n",
    "# Number of users with at least 5 items\n",
    "num_users = user_counts.count()\n",
    "print(f\"Total users with at least 5 items: {num_users}\")\n",
    "\n",
    "# Number of items with at least 5 users\n",
    "num_items = item_counts.count()\n",
    "print(f\"Total items with at least 5 users: {num_items}\")\n",
    "# Show user counts\n",
    "print(\"User counts with >= 5 items:\")\n",
    "user_counts.show()\n",
    "\n",
    "# Show item counts\n",
    "print(\"Item counts with >= 5 users:\")\n",
    "item_counts.show()\n",
    "\n",
    "filtered_df = ratings_df.join(user_counts, \"user\").join(item_counts, \"item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317c1b57",
   "metadata": {},
   "source": [
    "## **ALS Model Training & Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7f2c759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/12 20:10:59 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/07/12 20:10:59 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/07/12 20:11:00 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "[Stage 321:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.4286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Fix users DataFrame to unwrap struct to string\n",
    "users = (\n",
    "    filtered_df.select(\"user\")\n",
    "    .distinct()\n",
    "    .rdd\n",
    "    .zipWithIndex()\n",
    "    .map(lambda x: (x[0][0], x[1]))  # extract string from Row\n",
    "    .toDF([\"user\", \"user_id\"])\n",
    ")\n",
    "\n",
    "# Fix items DataFrame similarly\n",
    "items = (\n",
    "    filtered_df.select(\"item\")\n",
    "    .distinct()\n",
    "    .rdd\n",
    "    .zipWithIndex()\n",
    "    .map(lambda x: (x[0][0], x[1]))\n",
    "    .toDF([\"item\", \"item_id\"])\n",
    ")\n",
    "\n",
    "# Join on correct columns (user and item are strings, so joins work)\n",
    "als_df = filtered_df.join(users, \"user\").join(items, \"item\")\n",
    "\n",
    "# Split dataset\n",
    "(training, test) = als_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# ALS model setup and fit\n",
    "als = ALS(userCol=\"user_id\", itemCol=\"item_id\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a09c1d",
   "metadata": {},
   "source": [
    "## **Generating and Storing Top-N Recommendations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfbb0fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_n = model.recommendForAllUsers(10)\n",
    "top_n_named = top_n.join(users, \"user_id\").select(\"user\", \"recommendations\")\n",
    "top_n_named.write.json(\"top_10_recommendations.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf50afe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model.save(\"/media/amazonratings/als_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170584cb",
   "metadata": {},
   "source": [
    "###  **Uploading to Azure Blob**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cc5e78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: az: not found\r\n"
     ]
    }
   ],
   "source": [
    "!az storage blob upload \\\n",
    "  --account-name amazondatastore \\\n",
    "  --container-name recommender-data \\\n",
    "  --name top_10_recommendations.json \\\n",
    "  --file top_10_recommendations.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7562ec8b",
   "metadata": {},
   "source": [
    "### Optional: Save to Azure SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7834648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"top_10_recommendations.json\")\n",
    "df_exploded = df.explode(\"recommendations\")\n",
    "df_exploded[\"item_id\"] = df_exploded[\"recommendations\"].apply(lambda x: x[\"item\"])\n",
    "df_exploded[\"score\"] = df_exploded[\"recommendations\"].apply(lambda x: x[\"rating\"])\n",
    "\n",
    "engine = create_engine(\"mssql+pyodbc://umais:password@amazonrecommendersql.database.windows.net/recommenderdb?driver=ODBC+Driver+17+for+SQL+Server\")\n",
    "df_exploded[[\"user\", \"item_id\", \"score\"]].to_sql(\"Recommendations\", con=engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4622fc68",
   "metadata": {},
   "source": [
    "# Conclusion & Next Steps\n",
    "\n",
    "In this Proposal I :\n",
    "\n",
    "- Outlined the project details and implementation plan\n",
    "- Processed a large-scale dataset using PySpark on Azure VM (Docker)\n",
    "- Trained and evaluated a collaborative filtering model using ALS\n",
    "- Generated personalized top-N recommendations\n",
    "- Saved the model on the mounted drive\n",
    "- Stored results in Azure Blob and SQL for scalable access\n",
    "\n",
    "#  Mork Work to do in the Final\n",
    "\n",
    "**Model Optimization and Tuning:**\n",
    "\n",
    "Further refine ALS hyperparameters and experiment with alternative recommendation algorithms (e.g., matrix factorization variants or deep learning approaches) to improve accuracy and scalability.\n",
    "\n",
    "**Develop and Deploy the Recommendation API:**\n",
    "\n",
    "Build the Flask API to serve personalized recommendations based on the trained ALS model. This includes endpoints for real-time querying, error handling, and security features.\n",
    "\n",
    "**Frontend Development:**\n",
    "\n",
    "Create and enhance the web application UI to visualize recommendations dynamically and improve user experience.\n",
    "\n",
    "**Real-Time Recommendations:**\n",
    "\n",
    "Implement streaming data ingestion and real-time model updates to provide up-to-date recommendations for users as new interactions occur.\n",
    "\n",
    "**API Enhancement:**\n",
    "\n",
    "Extend the Flask API with authentication, rate limiting, and caching layers to improve security, scalability, and response times.\n",
    "\n",
    "**Frontend Improvements:**\n",
    "Enhance the web application with advanced visualization features, user personalization, and responsive design for better user engagement.\n",
    "\n",
    "**Robust Deployment:**\n",
    "\n",
    "Automate deployment pipelines using Azure DevOps or GitHub Actions, implement monitoring and alerting for production readiness, and ensure high availability with load balancing.\n",
    "\n",
    "**Security and Compliance:**\n",
    "\n",
    "Strengthen network security using Azure Private Endpoints, implement data encryption at rest and in transit, and ensure compliance with relevant data protection standards.\n",
    "\n",
    "**Scalability and Cost Optimization:**\n",
    "\n",
    "Explore container orchestration with Azure Kubernetes Service (AKS) for better scalability and manage costs using Azure Cost Management tools.\n",
    "\n",
    "## **Final Deliverable**\n",
    "\n",
    "- Jupyter Notebook on github with link of the recording and project implementation details.\n",
    "- Powerpoint presentation recorded to demonstrate the working recommender system.\n",
    "- Submission of project 6 and Final Project with above .ipynb file including all the work\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
