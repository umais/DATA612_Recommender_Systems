{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed59ade7",
   "metadata": {},
   "source": [
    "# **Final Project & Project 6**\n",
    "\n",
    "## **Applying Spark-Based Collaborative Filtering on Amazonâ€™s Electronics Reviews and creating a Personal Recommender System**\n",
    "\n",
    "**Name:** Umais Siddiqui  \n",
    "**Class:** Data 612 - Recommender Systems  \n",
    "**Github Link:** https://github.com/umais/DATA612_Recommender_Systems/blob/master/FinalProject_Project6/app/FinalProject.ipynb\n",
    "\n",
    "In this project I will build a personalized recommender system using Amazonâ€™s Electronics review dataset. It will apply Spark-based collaborative filtering (ALS) to generate product recommendations for users and deploys a real-time API using Flask and Azure Kubernetes Service (AKS). All data and results are stored in Azure File Storage using Azure Storage.\n",
    "\n",
    "**Environment Setup**:  \n",
    "The system is running inside a Docker container hosted on an Azure Virtual Machine. Azure File Share is mounted on the VM to persist intermediate files and logs. Azure Blob is used for scalable object storage, and Azure SQL is used to store structured results. This notebook is also being accessed from the Azure VM container.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508d223e",
   "metadata": {},
   "source": [
    "##  **Dataset Overview**\n",
    "\n",
    "- **Source**: Amazon Reviews (Electronics)\n",
    "- **Size**: ~7 million reviews\n",
    "- **Columns Used**: `reviewerID`, `asin`, `overall (rating)`, `reviewText`\n",
    "- **Goal**: Generate Top-N recommendations per user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b61d5c",
   "metadata": {},
   "source": [
    "# **Project Description and Deliverable**\n",
    "\n",
    "\n",
    "The goal of this project is to build a scalable and efficient recommender system using Apache Sparkâ€™s Alternating Least Squares (ALS) algorithm, hosted on Microsoft Azure infrastructure. The project workflow involves the following.\n",
    "\n",
    "- creating a new Azure Virtual Machine (VM) that is configured to access data stored in an Azure File Share via a mounted drive.\n",
    "- Within the VM, Docker will be installed to run Spark inside a containerized environment, ensuring portability and ease of management. \n",
    "\n",
    "\n",
    "- The Spark application will load data directly from the mounted Azure File Share, enabling seamless interaction with persistent cloud storage.\n",
    "\n",
    "\n",
    "- The recommender system will be developed by applying and iteratively improving the ALS model on the dataset to enhance its prediction accuracy and overall performance. Once the model achieves satisfactory results, it will be saved for production use.\n",
    "\n",
    "\n",
    "- To expose the recommendations for real-time usage, an API service will be created, providing endpoints for retrieving personalized product recommendations. \n",
    "\n",
    "\n",
    "- A user-friendly frontend web application will be built using Flask, serving as a visualization layer to display recommendations interactively to end-users.\n",
    "\n",
    "\n",
    "- The entire system, including the API and frontend, will be deployed and hosted on Azure, leveraging the cloud platformâ€™s scalability and reliability.\n",
    "\n",
    "## **Final Deliverable**\n",
    "\n",
    "- Jupyter Notebook on github\n",
    "- Powerpoint presentation recorded to demonstrate the working recommender system.\n",
    "\n",
    "# **Alignment with Project 6 Requirements: Hands-on with Microsoft Azure**\n",
    "\n",
    "This final recommender system project will fully incorporate the essential Azure components specified in Project 6, demonstrating practical experience with deploying a cloud solution on Microsoft Azure:\n",
    "\n",
    "**1. Persistent Storage**\n",
    "\n",
    "The project uses Azure File Share, which is part of Azure Storage, to store datasets and the trained ALS model.\n",
    "\n",
    "This provides long-term, durable, and scalable storage for both input data and model artifacts.\n",
    "\n",
    "The mounted Azure File Share is accessed both by the Azure VM running Spark inside Docker and by the Flask application hosted on Azure App Service, ensuring consistent data availability.\n",
    "\n",
    "**2. Compute Resource**\n",
    "\n",
    "The project provisions an Azure Virtual Machine that runs Docker and hosts Apache Spark inside containers for data processing, model training, and evaluation.\n",
    "\n",
    "The trained ALS model is then served through a Flask web application deployed on Azure App Service, a fully managed platform-as-a-service (PaaS) compute environment.\n",
    "\n",
    "This multi-tier compute architecture showcases the use of different Azure compute options: IaaS (VM) and PaaS (App Service).\n",
    "\n",
    "**3. Network Security**\n",
    "\n",
    "The Azure VM is placed within an Azure Virtual Network (VNet) to isolate and secure network traffic.\n",
    "\n",
    "Appropriate Network Security Groups (NSGs) are configured to restrict inbound and outbound access, ensuring only authorized users and services can connect to the VM and Azure File Share.\n",
    "\n",
    "Similarly, the Azure App Service uses private endpoints or VNet integration to securely access the Azure File Share and backend resources.\n",
    "\n",
    "This setup enforces secure communication channels between compute and storage services, complying with best practices for cloud security.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19c5da",
   "metadata": {},
   "source": [
    "# Initial Setup, Data Cleaning, Analysis, and Model Generation\n",
    "\n",
    "The project begins with setting up the environment on an Azure Virtual Machine, where Docker is installed and configured to run Apache Spark in a containerized environment. The dataset, stored on an Azure File Share and mounted inside the container, is accessed directly for processing.\n",
    "\n",
    "# Data Cleaning and Exploration\n",
    "The raw dataset is loaded into a Spark DataFrame. As can be seen below\n",
    "\n",
    "**Initial data cleaning steps are performed, including:**\n",
    "\n",
    "- Selecting relevant columns (e.g., user IDs, item IDs, and ratings).\n",
    "\n",
    "- Dropping rows with missing values to ensure data quality.\n",
    "\n",
    "- Filtering users and items to retain only those with sufficient interaction counts (e.g., users with at least 5 ratings and items rated by at least 5 users).\n",
    "\n",
    "Exploratory data analysis is conducted to understand the distribution of ratings, user behavior, and item popularity, which informs model tuning decisions.\n",
    "\n",
    "**Model Building**\n",
    "The cleaned dataset is then used to train a collaborative filtering model using Sparkâ€™s ALS (Alternating Least Squares) algorithm.\n",
    "\n",
    "The model training includes hyperparameter tuning and iterative improvements aimed at enhancing recommendation accuracy.\n",
    "\n",
    "Upon achieving satisfactory performance metrics, the model is saved to the mounted Azure File Share for persistence and later use by the flask application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a69a822a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/14 04:46:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/14 04:46:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/14 04:46:06 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n",
      "+--------------+----------+-------+\n",
      "|    reviewerID|      asin|overall|\n",
      "+--------------+----------+-------+\n",
      "| AO94DHGC771SJ|0528881469|    5.0|\n",
      "| AMO214LNFCEI4|0528881469|    1.0|\n",
      "|A3N7T0DY83Y4IG|0528881469|    3.0|\n",
      "|A1H8PY3QHMQQA0|0528881469|    2.0|\n",
      "|A24EV6RXELQZ63|0528881469|    1.0|\n",
      "+--------------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Initialize Spark session with proper Jetty JARs\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"AmazonRecommender\")\n",
    "    .config(\"spark.driver.extraClassPath\",\n",
    "            \"/opt/spark-3.4.1-bin-hadoop3/jars/jetty-6.1.26.jar:\" +\n",
    "            \"/opt/spark-3.4.1-bin-hadoop3/jars/jetty-util-6.1.26.jar:\" +\n",
    "            \"/opt/spark-3.4.1-bin-hadoop3/jars/jetty-ajax-6.1.26.jar\")\n",
    "    .config(\"spark.executor.extraClassPath\",\n",
    "            \"/opt/spark-3.4.1-bin-hadoop3/jars/jetty-6.1.26.jar:\" +\n",
    "            \"/opt/spark-3.4.1-bin-hadoop3/jars/jetty-util-6.1.26.jar:\" +\n",
    "            \"/opt/spark-3.4.1-bin-hadoop3/jars/jetty-ajax-6.1.26.jar\")\n",
    "    .config(\"spark.pyspark.python\", \"/usr/local/bin/python3\")\n",
    "    .config(\"spark.pyspark.driver.python\", \"/usr/local/bin/python3\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "# Read data from the mounted Azure File Share directory\n",
    "df = spark.read.json(\"/media/amazonratings/Electronics_5.json\")\n",
    "\n",
    "# Show schema and sample data\n",
    "df.printSchema()\n",
    "df.select(\"reviewerID\", \"asin\", \"overall\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18670200",
   "metadata": {},
   "source": [
    "## **Data Cleaning & Filtering**\n",
    "\n",
    "To reduce noise and improve model performance:\n",
    "- Users with fewer than 5 reviews are excluded\n",
    "- Items with fewer than 5 reviews are excluded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d85cbe6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users with at least 5 items: 192403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items with at least 5 users: 63001\n",
      "User counts with >= 5 items:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|          user|user_count|\n",
      "+--------------+----------+\n",
      "|A18FTRFQQ141CP|         5|\n",
      "|A2GPNXFUUV51ZZ|         7|\n",
      "|A15K7HV1XD6YWR|         8|\n",
      "|A3PDGWYC08DXF4|        15|\n",
      "|  A44UKZE6XEV9|        16|\n",
      "|A3FE9EUVTU3UD8|         8|\n",
      "|A3DKP8M0GSP8UK|        26|\n",
      "|A37LCWTTQMBMFX|         6|\n",
      "|A141E91QV31KER|         6|\n",
      "|A1SCWY8O0IL2HU|        29|\n",
      "| AWWBZZXN32I6H|         7|\n",
      "|A1PG70NH85K859|        34|\n",
      "|A1IJOBQD8CY8K1|        25|\n",
      "|A2690TEJA2N778|        11|\n",
      "|A3GHZZM7CNK77I|         5|\n",
      "|A3TPM2VJA0X1Y2|         5|\n",
      "|A1MRESWHA86B5B|         5|\n",
      "|A2X8NZUNAWX9SO|         7|\n",
      "|A2WY7M2G4FUK9Y|         8|\n",
      "|A2JL1GIC0JAFW9|        15|\n",
      "+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Item counts with >= 5 users:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      item|item_count|\n",
      "+----------+----------+\n",
      "|B00000J3Q1|        14|\n",
      "|B00001W0DC|        15|\n",
      "|B00003OPEV|         6|\n",
      "|B00005853W|         5|\n",
      "|B00005Q5U5|        33|\n",
      "|B00005T3Z7|        16|\n",
      "|B000068UY7|        24|\n",
      "|B00006JLOT|        18|\n",
      "|B000083GPS|         5|\n",
      "|B00008WIX2|         6|\n",
      "|B00008ZPN3|         8|\n",
      "|B00009R6FQ|         5|\n",
      "|B0000AKACN|         9|\n",
      "|B0000E6FY7|         5|\n",
      "|B0000UV0IQ|         8|\n",
      "|B0001CLYAW|         6|\n",
      "|B00021EE4U|        67|\n",
      "|B00021Z98A|        14|\n",
      "|B0002D05RI|         5|\n",
      "|B0002D6PNQ|         8|\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "ratings_df = df.selectExpr(\"reviewerID as user\", \"asin as item\", \"overall as rating\").dropna()\n",
    "user_counts = ratings_df.groupBy(\"user\").agg(count(\"item\").alias(\"user_count\")).filter(\"user_count >= 5\")\n",
    "item_counts = ratings_df.groupBy(\"item\").agg(count(\"user\").alias(\"item_count\")).filter(\"item_count >= 5\")\n",
    "\n",
    "# Number of users with at least 5 items\n",
    "num_users = user_counts.count()\n",
    "print(f\"Total users with at least 5 items: {num_users}\")\n",
    "\n",
    "# Number of items with at least 5 users\n",
    "num_items = item_counts.count()\n",
    "print(f\"Total items with at least 5 users: {num_items}\")\n",
    "# Show user counts\n",
    "print(\"User counts with >= 5 items:\")\n",
    "user_counts.show()\n",
    "\n",
    "# Show item counts\n",
    "print(\"Item counts with >= 5 users:\")\n",
    "item_counts.show()\n",
    "\n",
    "filtered_df = ratings_df.join(user_counts, \"user\").join(item_counts, \"item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317c1b57",
   "metadata": {},
   "source": [
    "## **ALS Model Training & Evaluation**\n",
    "\n",
    "This PySpark code builds a collaborative filtering recommendation system using the Alternating Least Squares (ALS) algorithm. Let's break it down, especially focusing on the ALS parameters.\n",
    "\n",
    "We will start of by using basic ALS \n",
    "\n",
    "### **ALS Model Setup**\n",
    "\n",
    "als = ALS(\n",
    "    userCol=\"user_id\",          # column for user ID (must be integer)\n",
    "    itemCol=\"item_id\",          # column for item ID (must be integer)\n",
    "    ratingCol=\"rating\",         # the observed rating\n",
    "    coldStartStrategy=\"drop\"    # drop predictions with missing user/item factors (unseen in training)\n",
    ")\n",
    "\n",
    "#### **ALS Parameters Explained:**\n",
    "\n",
    "- **userCol / itemCol:** Specifies which columns represent users and items.\n",
    "\n",
    "- **ratingCol:** The column that holds the userâ€™s rating of the item.\n",
    "\n",
    "- **coldStartStrategy=\"drop\":** If a user or item in the test set wasnâ€™t seen in the training set, ALS canâ€™t predict for it. This option drops such rows to avoid NaN in predictions.\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "- This code builds a collaborative filtering recommender.\n",
    "\n",
    "- Converts user/item strings â†’ numeric IDs for ALS compatibility.\n",
    "\n",
    "- Trains ALS on 80% of the data.\n",
    "\n",
    "- Uses ALS parameters like userCol, itemCol, ratingCol, and coldStartStrategy.\n",
    "\n",
    "- Evaluates the model using RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7f2c759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 631:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.4311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Fix users DataFrame to unwrap struct to string\n",
    "users = (\n",
    "    filtered_df.select(\"user\")\n",
    "    .distinct()\n",
    "    .rdd\n",
    "    .zipWithIndex()\n",
    "    .map(lambda x: (x[0][0], x[1]))  # extract string from Row\n",
    "    .toDF([\"user\", \"user_id\"])\n",
    ")\n",
    "\n",
    "# Fix items DataFrame similarly\n",
    "items = (\n",
    "    filtered_df.select(\"item\")\n",
    "    .distinct()\n",
    "    .rdd\n",
    "    .zipWithIndex()\n",
    "    .map(lambda x: (x[0][0], x[1]))\n",
    "    .toDF([\"item\", \"item_id\"])\n",
    ")\n",
    "\n",
    "# Join on correct columns (user and item are strings, so joins work)\n",
    "als_df = filtered_df.join(users, \"user\").join(items, \"item\")\n",
    "\n",
    "# Split dataset\n",
    "(training, test) = als_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# ALS model setup and fit\n",
    "als = ALS(userCol=\"user_id\", itemCol=\"item_id\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eea41f",
   "metadata": {},
   "source": [
    "### **Explaining the results and further Improvements**\n",
    "\n",
    "**What does RMSE: 1.4384 mean?**\n",
    "\n",
    "The RMSE (Root Mean Square Error) measures how far off your predicted ratings are from the actual user ratings.\n",
    "\n",
    "In this case, RMSE: 1.4384 means that on average, the predictions are off by about 1.44 rating points.\n",
    "\n",
    "If your ratings are on a scale of 1 to 5 (as in most recommendation datasets), this is moderately accurate, but not optimal.\n",
    "\n",
    "Lower RMSE = better accuracy.\n",
    "\n",
    "### How are we improving the ALS model in the code below?\n",
    "\n",
    "In new code block below, I will be explicitly tuning three new ALS hyperparameters not used before that directly affect model performance:\n",
    "\n",
    "**1. rank=20:**\n",
    "\n",
    "**What it does:** Controls the number of latent factors used to represent users and items.\n",
    "\n",
    "**Why it matters:** A higher rank allows the model to capture more subtle patterns in user-item interactions, at the cost of more computation and potential overfitting.\n",
    "\n",
    "Default is 10, so I am doubling the capacity.\n",
    "\n",
    "**2. maxIter=15:**\n",
    "\n",
    "**What it does:** Sets the number of iterations for ALS to alternate between user and item factor updates.\n",
    "\n",
    "**Why it matters:** More iterations can help the model converge to a better solution â€” especially useful on larger or sparser datasets.\n",
    "\n",
    "Default is 10, so this gives more training time for better convergence.\n",
    "\n",
    "**3. regParam=0.1:**\n",
    "\n",
    "**What it does:** Adds regularization to avoid overfitting.\n",
    "\n",
    "**Why it matters:** Too small = overfitting; too large = underfitting.\n",
    "\n",
    "I am tuning it from the previous version (which used the default 0.1 or didn't specify), and it's often one of the most impactful parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "216ba7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1112:>                                                       (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.2713\n",
      "Test RMSE: 1.3252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Unwrap RDD rows to plain tuples for users\n",
    "users = (\n",
    "    filtered_df.select(\"user\")\n",
    "    .distinct()\n",
    "    .rdd\n",
    "    .zipWithIndex()\n",
    "    .map(lambda x: (x[0][0], x[1]))\n",
    "    .toDF([\"user\", \"user_id\"])\n",
    ")\n",
    "\n",
    "# Same for items\n",
    "items = (\n",
    "    filtered_df.select(\"item\")\n",
    "    .distinct()\n",
    "    .rdd\n",
    "    .zipWithIndex()\n",
    "    .map(lambda x: (x[0][0], x[1]))\n",
    "    .toDF([\"item\", \"item_id\"])\n",
    ")\n",
    "\n",
    "# Join to get numeric IDs\n",
    "als_df = filtered_df.join(users, \"user\").join(items, \"item\")\n",
    "\n",
    "# Remove rows with nulls in any column (optional but recommended)\n",
    "als_df = als_df.na.drop(subset=[\"user_id\", \"item_id\", \"rating\"])\n",
    "\n",
    "# Split dataset\n",
    "(training, test) = als_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# ALS model with tuned params\n",
    "als = ALS(\n",
    "    userCol=\"user_id\",\n",
    "    itemCol=\"item_id\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    rank=20,\n",
    "    maxIter=15,\n",
    "    regParam=0.1\n",
    ")\n",
    "\n",
    "\n",
    "# Train model\n",
    "model = als.fit(training)\n",
    "\n",
    "# Predictions and evaluation\n",
    "train_preds = model.transform(training)\n",
    "test_preds = model.transform(test)\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "train_rmse = evaluator.evaluate(train_preds)\n",
    "test_rmse = evaluator.evaluate(test_preds)\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# Optionally save model\n",
    "# model.save(\"/path/to/save/als_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38041764",
   "metadata": {},
   "source": [
    "### Discuss Improved results \n",
    "\n",
    "âœ… Train RMSE: 0.2710\n",
    "This is very low, indicating that the model is fitting the training data extremely well.\n",
    "\n",
    "It means that on average, the predicted ratings are only ~0.27 points away from the true ratings in the training data.\n",
    "\n",
    "âš ï¸ Test RMSE: 1.3237\n",
    "This is an improvement over your previous test RMSE of 1.4384, so your model generalization is getting better.\n",
    "\n",
    "However, the gap between train (0.27) and test (1.32) is fairly large, which could be a sign of overfitting â€” the model learns training data too well but doesnâ€™t generalize as strongly to unseen data.\n",
    "\n",
    "### Further Improvement below \n",
    "\n",
    "In code below we are further doing HyperparameterTuning we will adjust the paramters as per below.\n",
    "\n",
    "rank=8: A more compact latent factor model than your earlier rank=20, potentially reducing overfitting while maintaining complexity.\n",
    "\n",
    "maxIter=20: More training passes to improve convergence.\n",
    "\n",
    "regParam=0.2: Stronger regularization than earlier 0.1, which reduces overfitting, especially useful if your training RMSE is much lower than test RMSE.\n",
    "\n",
    "This combination may provide a better trade-off between model expressiveness and generalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1f12874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1621:>                                                       (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.5278\n",
      "Test RMSE: 1.2875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Unwrap RDD rows to plain tuples for users\n",
    "users = (\n",
    "    filtered_df.select(\"user\")\n",
    "    .distinct()\n",
    "    .rdd\n",
    "    .zipWithIndex()\n",
    "    .map(lambda x: (x[0][0], x[1]))\n",
    "    .toDF([\"user\", \"user_id\"])\n",
    ")\n",
    "\n",
    "# Same for items\n",
    "items = (\n",
    "    filtered_df.select(\"item\")\n",
    "    .distinct()\n",
    "    .rdd\n",
    "    .zipWithIndex()\n",
    "    .map(lambda x: (x[0][0], x[1]))\n",
    "    .toDF([\"item\", \"item_id\"])\n",
    ")\n",
    "\n",
    "# Join to get numeric IDs\n",
    "als_df = filtered_df.join(users, \"user\").join(items, \"item\")\n",
    "\n",
    "# Remove rows with nulls in any column (optional but recommended)\n",
    "als_df = als_df.na.drop(subset=[\"user_id\", \"item_id\", \"rating\"])\n",
    "\n",
    "# Split dataset\n",
    "(training, test) = als_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# ALS model with tuned params\n",
    "\n",
    "als = ALS(\n",
    "    userCol=\"user_id\",\n",
    "    itemCol=\"item_id\",\n",
    "    ratingCol=\"rating\",\n",
    "    rank=8,\n",
    "    maxIter=20,\n",
    "    regParam=0.2,\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model = als.fit(training)\n",
    "\n",
    "# Predictions and evaluation\n",
    "train_preds = model.transform(training)\n",
    "test_preds = model.transform(test)\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "train_rmse = evaluator.evaluate(train_preds)\n",
    "test_rmse = evaluator.evaluate(test_preds)\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# Optionally save model\n",
    "# model.save(\"/path/to/save/als_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366aa6cc",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "**Train RMSE: 0.5278**\n",
    "The model fits the training data very well.\n",
    "\n",
    "A low value here suggests that the model is capturing meaningful user-item relationships.\n",
    "\n",
    "**Test RMSE: 1.2875**\n",
    "This is a solid improvement over my earlier test RMSEs:\n",
    "\n",
    "Previous: ~1.4384, then 1.3237\n",
    "\n",
    "**Now:** 1.2875\n",
    "\n",
    "It means that, on average, predictions are off by about 1.29 rating points on unseen data.\n",
    "\n",
    "The gap between train and test RMSE is reasonable, indicating controlled overfitting.\n",
    "\n",
    "## Further Enhancement\n",
    "\n",
    "In the code below we will further improve the ALS recommendation model by first filtering out inactive users and unpopular items to reduce data sparsity and noise, which enhances training stability and model accuracy. It then systematically performs a manual grid search over key hyperparametersâ€”rank and regularizationâ€”to empirically find the best combination that minimizes test RMSE, ensuring better generalization. Additionally, by enforcing nonnegative latent factors, the model gains improved interpretability and can avoid overfitting. Tracking and selecting the best model based on evaluation results further ensures optimal performance before deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0a36769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ALS model with rank=8, regParam=0.05...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/14 05:14:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:14:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:15:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:15:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:15:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:15:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:15:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:15:44 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:17:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:17:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:17:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:17:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:17:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:17:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE = 1.4056\n",
      "Training ALS model with rank=8, regParam=0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/14 05:18:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:18:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:18:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:18:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:18:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:18:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:19:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:19:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:19:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:19:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:19:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:19:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE = 1.3342\n",
      "Training ALS model with rank=8, regParam=0.15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/14 05:20:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:20:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:20:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:20:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:21:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:21:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:21:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE = 1.2916\n",
      "Training ALS model with rank=12, regParam=0.05...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/14 05:22:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:22:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:22:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:22:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:22:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:22:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:24:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:24:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:24:26 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:24:26 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:24:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:24:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE = 1.3607\n",
      "Training ALS model with rank=12, regParam=0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/14 05:25:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:25:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:25:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:25:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:26:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:26:54 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:26:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:26:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:26:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:26:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE = 1.3112\n",
      "Training ALS model with rank=12, regParam=0.15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/14 05:27:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:27:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:27:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:27:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:29:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:29:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:29:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:29:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:29:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:29:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE = 1.2795\n",
      "Training ALS model with rank=20, regParam=0.05...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/14 05:30:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:30:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:30:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:30:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:32:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:32:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:32:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:32:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:32:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:32:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE = 1.3120\n",
      "Training ALS model with rank=20, regParam=0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/14 05:33:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:33:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:33:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:33:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:33:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:33:28 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:35:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:35:27 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:35:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:35:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:35:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:35:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE = 1.2868\n",
      "Training ALS model with rank=20, regParam=0.15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/14 05:36:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:36:20 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:36:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:36:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:36:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:36:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:38:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:38:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:38:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:38:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:38:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:38:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE = 1.2665\n",
      "\n",
      "âœ… Best model parameters:\n",
      "{'rank': 20, 'regParam': 0.15}\n",
      "âœ… Best Test RMSE: 1.2665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/14 05:39:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:39:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:39:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:39:14 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:39:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/07/14 05:39:15 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 4866:>                                                       (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Train RMSE: 0.3953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4866:============================>                           (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# STEP 1: Filter active users and popular items\n",
    "user_counts = filtered_df.groupBy(\"user\").agg(count(\"item\").alias(\"user_count\"))\n",
    "item_counts = filtered_df.groupBy(\"item\").agg(count(\"user\").alias(\"item_count\"))\n",
    "\n",
    "filtered_users = user_counts.filter(col(\"user_count\") >= 5)\n",
    "filtered_items = item_counts.filter(col(\"item_count\") >= 5)\n",
    "\n",
    "filtered_df = (\n",
    "    filtered_df.join(filtered_users, \"user\")\n",
    "               .join(filtered_items, \"item\")\n",
    "               .select(\"user\", \"item\", \"rating\")\n",
    ")\n",
    "\n",
    "# STEP 2: Encode users and items to IDs\n",
    "users = (\n",
    "    filtered_df.select(\"user\").distinct().rdd.zipWithIndex()\n",
    "    .map(lambda x: (x[0][0], x[1])).toDF([\"user\", \"user_id\"])\n",
    ")\n",
    "items = (\n",
    "    filtered_df.select(\"item\").distinct().rdd.zipWithIndex()\n",
    "    .map(lambda x: (x[0][0], x[1])).toDF([\"item\", \"item_id\"])\n",
    ")\n",
    "\n",
    "als_df = (\n",
    "    filtered_df.join(users, \"user\").join(items, \"item\")\n",
    "               .select(\"user_id\", \"item_id\", \"rating\")\n",
    ")\n",
    "\n",
    "# STEP 3: Split data\n",
    "training, test = als_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# STEP 4: Initialize evaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# STEP 5: Try different hyperparameters (manual grid search)\n",
    "ranks = [8, 12, 20]\n",
    "regParams = [0.05, 0.1, 0.15]\n",
    "maxIter = 15\n",
    "\n",
    "best_rmse = float(\"inf\")\n",
    "best_model = None\n",
    "best_params = {}\n",
    "\n",
    "for rank in ranks:\n",
    "    for reg in regParams:\n",
    "        print(f\"Training ALS model with rank={rank}, regParam={reg}...\")\n",
    "        als = ALS(\n",
    "            userCol=\"user_id\",\n",
    "            itemCol=\"item_id\",\n",
    "            ratingCol=\"rating\",\n",
    "            coldStartStrategy=\"drop\",\n",
    "            rank=rank,\n",
    "            maxIter=maxIter,\n",
    "            regParam=reg,\n",
    "            nonnegative=True\n",
    "        )\n",
    "        model = als.fit(training)\n",
    "        predictions = model.transform(test)\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "        print(f\"Test RMSE = {rmse:.4f}\")\n",
    "\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model = model\n",
    "            best_params = {\"rank\": rank, \"regParam\": reg}\n",
    "\n",
    "# STEP 6: Print final results\n",
    "print(\"\\nâœ… Best model parameters:\")\n",
    "print(best_params)\n",
    "print(f\"âœ… Best Test RMSE: {best_rmse:.4f}\")\n",
    "\n",
    "# (Optional) Evaluate on training set\n",
    "train_predictions = best_model.transform(training)\n",
    "train_rmse = evaluator.evaluate(train_predictions)\n",
    "print(f\"ðŸ“ˆ Train RMSE: {train_rmse:.4f}\")\n",
    "\n",
    "# (Optional) Save model\n",
    "# best_model.save(\"/media/amazonratings/ALS_Model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac86a25",
   "metadata": {},
   "source": [
    "The latest results **Test RMSE: 1.2665** and **Train RMSE: 0.3953**  show a solid model fit: the training error is fairly low, meaning the model learns well from the training data, and the test error has improved, indicating better generalization to unseen data. The gap between train and test RMSE suggests some degree of overfitting, but itâ€™s reasonably controlled given the complexity of recommendation tasks. Overall, these numbers reflect that your improvementsâ€”like filtering sparse users/items and hyperparameter tuningâ€”are effectively enhancing the ALS modelâ€™s predictive performance.\n",
    "\n",
    "## **What This Means**\n",
    "\n",
    "- Test RMSE of 1.2662 with rank=20 and regParam=0.15 is a solid result for an implicit collaborative filtering model like ALS, especially on sparse datasets like Amazon Electronics reviews\n",
    "\n",
    "- RMSE (Root Mean Squared Error) measures how close predicted ratings are to actual ratings.\n",
    "\n",
    "- A lower RMSE indicates better accuracy; anything around 1.2â€“1.3 on Amazon review datasets (with many users/items and sparse ratings) is quite acceptable.\n",
    "\n",
    "- I have tuned the model and found effective parameters this is a good sign that ALS is learning meaningful latent factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a09c1d",
   "metadata": {},
   "source": [
    "## **Generating and Storing Top-N Recommendations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfbb0fc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5087:==========================================>             (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|user          |recommendations                                                                                                                                                                                      |\n",
      "+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|A2CCHCGJY0IR9F|[{102, 4.6342835}, {47339, 4.5334196}, {13286, 4.5038705}, {13026, 4.4762344}, {19722, 4.458829}, {39501, 4.4353642}, {31967, 4.4226546}, {4345, 4.405386}, {53272, 4.376808}, {2887, 4.371308}]     |\n",
      "|AX4IT87INWPV  |[{60448, 5.227199}, {20606, 5.2210717}, {43524, 5.2188935}, {16405, 5.197286}, {17124, 5.170926}, {11986, 5.1477776}, {24754, 5.126092}, {36990, 5.1249676}, {62640, 5.1241374}, {38248, 5.1142125}] |\n",
      "|AN7V23UTYP5VE |[{56693, 5.580687}, {22394, 5.5036235}, {45687, 5.1198735}, {11683, 5.071412}, {26504, 5.024417}, {4327, 5.0213594}, {37006, 5.0151806}, {51899, 5.0082445}, {19500, 4.9922104}, {14558, 4.983018}]  |\n",
      "|A3BD2D7UVZK9UJ|[{32755, 4.364942}, {26906, 4.356136}, {32284, 4.051995}, {1071, 4.046435}, {32727, 4.0153933}, {39150, 4.001938}, {579, 4.0013027}, {51394, 3.9806135}, {60783, 3.9228027}, {1907, 3.9094524}]      |\n",
      "|A1PEQK2PFKPCDQ|[{56706, 5.247127}, {24086, 5.22482}, {55157, 5.1463876}, {47944, 5.1305833}, {54018, 5.121816}, {3017, 5.0977254}, {47346, 5.09396}, {5089, 5.0876894}, {15275, 5.0641093}, {47685, 5.0470805}]     |\n",
      "|A20KASXNRIXS21|[{60448, 4.8465486}, {28849, 4.678357}, {10707, 4.5720587}, {37183, 4.5645967}, {46045, 4.5644126}, {20703, 4.555257}, {42750, 4.5434127}, {8802, 4.5397615}, {34042, 4.538959}, {47263, 4.538078}]  |\n",
      "|A56WSPM8GAOUK |[{32625, 5.8245606}, {60448, 5.7688107}, {46056, 5.7550063}, {47944, 5.719843}, {6940, 5.710074}, {25944, 5.6703563}, {38248, 5.6527104}, {2910, 5.643195}, {49272, 5.6356635}, {20606, 5.6316137}]  |\n",
      "|A1EU66488BUQC2|[{7654, 4.43017}, {26598, 4.418319}, {55991, 4.383336}, {34026, 4.3711057}, {36063, 4.3108993}, {42893, 4.3096886}, {40547, 4.278793}, {13396, 4.2741847}, {342, 4.2647257}, {2771, 4.2583075}]      |\n",
      "|A1XNVGDPDEK2NW|[{60448, 5.887595}, {56693, 5.764101}, {11108, 5.7171035}, {356, 5.6115694}, {3794, 5.5941796}, {19746, 5.5694017}, {46242, 5.5677843}, {32995, 5.5655026}, {10554, 5.5361557}, {44245, 5.521473}]   |\n",
      "|AUQPP5X315VRM |[{50705, 5.472794}, {34389, 5.3578053}, {60448, 5.323419}, {6197, 5.2736664}, {3645, 5.2200327}, {5633, 5.1924925}, {11197, 5.190712}, {27395, 5.1882524}, {2400, 5.166984}, {30270, 5.157432}]      |\n",
      "|A24FNJGU8UOGXC|[{32625, 3.914648}, {3794, 3.869724}, {6940, 3.773673}, {60448, 3.7607925}, {3645, 3.730129}, {38248, 3.7253857}, {29911, 3.7073376}, {58368, 3.691669}, {26494, 3.688651}, {33859, 3.684393}]       |\n",
      "|A1DOCBKTWHI7PV|[{31766, 5.3385124}, {49630, 5.318446}, {3794, 5.1565666}, {13715, 5.113348}, {22318, 5.0786676}, {41194, 5.04953}, {43566, 5.0357723}, {4537, 5.0354023}, {40981, 5.0046277}, {6822, 4.9954934}]    |\n",
      "|A28RNCMMYEZYM0|[{20103, 5.424498}, {131, 5.398235}, {60849, 5.3303146}, {56706, 5.3300285}, {56237, 5.226089}, {31672, 5.208238}, {4244, 5.2018704}, {30215, 5.200211}, {41546, 5.1909757}, {4381, 5.159302}]       |\n",
      "|A3E13IZ6WIPZWN|[{4614, 4.866169}, {8378, 4.8473353}, {25944, 4.8272038}, {239, 4.817756}, {10554, 4.8048253}, {32625, 4.7903237}, {60448, 4.7864437}, {29179, 4.7677727}, {1240, 4.746383}, {31672, 4.7431145}]     |\n",
      "|A228X4MOQ31B9C|[{49630, 5.4855895}, {20606, 5.265812}, {60849, 5.207452}, {60448, 5.20112}, {4381, 5.182676}, {18210, 5.151827}, {34550, 5.1445093}, {3746, 5.138081}, {31672, 5.134499}, {17510, 5.1327224}]       |\n",
      "|A2PJCFUE5QKE53|[{46143, 3.7524097}, {4614, 3.5668755}, {23193, 3.5442524}, {47825, 3.4963367}, {60220, 3.4791253}, {48577, 3.4770982}, {3794, 3.468881}, {45306, 3.4648035}, {16335, 3.4612398}, {60448, 3.4329572}]|\n",
      "|A359T9R7B5QU10|[{50461, 5.548609}, {61216, 5.5410285}, {39697, 5.4632616}, {11548, 5.3556924}, {2862, 5.3471594}, {10861, 5.326465}, {30131, 5.309269}, {59895, 5.270103}, {8286, 5.248889}, {25200, 5.2481985}]    |\n",
      "|AS0CYBAN6EM06 |[{60448, 5.473779}, {44351, 5.382133}, {21237, 5.3239317}, {19746, 5.3176837}, {18779, 5.3053412}, {49630, 5.3027196}, {56693, 5.296655}, {10554, 5.2777476}, {32625, 5.2715087}, {356, 5.2673364}]  |\n",
      "|AA86JVS21K3SJ |[{8378, 5.57524}, {60448, 5.471957}, {9128, 5.3348007}, {17039, 5.331034}, {20324, 5.3214045}, {30395, 5.309883}, {29206, 5.3059897}, {4080, 5.302842}, {5110, 5.2946687}, {812, 5.2941995}]         |\n",
      "|A3FAV9KE01OVPR|[{60448, 5.633736}, {29911, 5.550186}, {49630, 5.5257435}, {36363, 5.5136204}, {3794, 5.5089307}, {3645, 5.480639}, {31672, 5.4252367}, {33859, 5.391895}, {56693, 5.3672905}, {45477, 5.364592}]    |\n",
      "+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# Get top 10 recommendations for all users (user_id, recommendations)\n",
    "top_n = model.recommendForAllUsers(10)\n",
    "\n",
    "# Join with users DataFrame to get original user names\n",
    "top_n_named = top_n.join(users, \"user_id\").select(\"user\", \"recommendations\")\n",
    "\n",
    "# Optional: Explode recommendations if you want one row per (user, item, rating)\n",
    "# exploded = top_n_named.select(\n",
    "#     \"user\",\n",
    "#     explode(\"recommendations\").alias(\"rec\")\n",
    "# ).select(\n",
    "#     \"user\",\n",
    "#     col(\"rec.item_id\"),\n",
    "#     col(\"rec.rating\")\n",
    "# )\n",
    "\n",
    "# Coalesce to 1 partition to write a single JSON file inside the folder\n",
    "top_n_named.coalesce(1).write.mode(\"overwrite\").json(\"/media/amazonratings/top_10_recommendations\")\n",
    "\n",
    "# To verify, you can show some rows\n",
    "top_n_named.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf50afe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model.write().overwrite().save(\"/media/amazonratings/als_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170584cb",
   "metadata": {},
   "source": [
    "#  **Creating the Flask API and UI**\n",
    "\n",
    "- Now that we have the Model Saved to the Azure File Share in Azure Storage. \n",
    "\n",
    "- The file Storage is mounted to our Azure VM \n",
    "\n",
    "- Within our container we will make use of the flask package in Python and create an API to make a call to prediction on the rating by passing the user id and item id \n",
    "\n",
    "- We will then build a UI and API below by first importing the neccesarry packages\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7415e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALSModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db91367c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Set Python paths before SparkSession is created\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30995d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/local/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/local/bin/python3\"\n",
    "\n",
    "app = Flask(__name__, template_folder='templates', static_folder='static')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63201cc9",
   "metadata": {},
   "source": [
    "# Initialize Spark session with explicit python config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d708ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " spark = SparkSession.builder \\\n",
    "    .appName(\"ALS Recommender API\") \\\n",
    "    .config(\"spark.pyspark.python\", \"/usr/local/bin/python3\") \\\n",
    "    .config(\"spark.pyspark.driver.python\", \"/usr/local/bin/python3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "MODEL_PATH = \"/media/amazonratings/als_model\"\n",
    "als_model = ALSModel.load(MODEL_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa897459",
   "metadata": {},
   "source": [
    "# Create the Endpoints for the UI and API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce01e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        user_id = int(request.form['user_id'])\n",
    "        item_id = int(request.form['item_id'])\n",
    "    except (KeyError, ValueError):\n",
    "        return render_template(\"index.html\", error=\"Invalid input\")\n",
    "\n",
    "    input_df = spark.createDataFrame([(user_id, item_id)], [\"user_id\", \"item_id\"])\n",
    "    prediction = als_model.transform(input_df).collect()\n",
    "\n",
    "    if prediction and prediction[0]['prediction'] is not None:\n",
    "        pred = round(prediction[0]['prediction'], 4)\n",
    "        return render_template(\"index.html\", prediction=pred, user_id=user_id, item_id=item_id)\n",
    "    else:\n",
    "        return render_template(\"index.html\", error=\"No prediction available\")\n",
    "\n",
    "@app.route('/api/hello', methods=['GET'])\n",
    "def hello():\n",
    "    return jsonify({\"message\": \"Hello, world!\"})\n",
    "\n",
    "@app.route('/api/predict', methods=['POST'])\n",
    "def api_predict():\n",
    "    data = request.get_json()\n",
    "\n",
    "    if not data or not all(k in data for k in ('user_id', 'item_id')):\n",
    "        return jsonify({'error': 'user_id and item_id are required'}), 400\n",
    "\n",
    "    try:\n",
    "        user_id = int(data['user_id'])\n",
    "        item_id = int(data['item_id'])\n",
    "    except ValueError:\n",
    "        return jsonify({'error': 'user_id and item_id must be integers'}), 400\n",
    "\n",
    "    input_df = spark.createDataFrame([(user_id, item_id)], [\"user_id\", \"item_id\"])\n",
    "    prediction = als_model.transform(input_df).collect()\n",
    "\n",
    "    if prediction and prediction[0]['prediction'] is not None:\n",
    "        pred = prediction[0]['prediction']\n",
    "        return jsonify({'user_id': user_id, 'item_id': item_id, 'prediction': round(pred, 4)})\n",
    "    else:\n",
    "        return jsonify({'error': 'No prediction available'}), 404\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host=\"0.0.0.0\", port=8080, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e6b676",
   "metadata": {},
   "source": [
    "# **Design the HTML Template for User Input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f7732",
   "metadata": {},
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>ALS Recommender</title>\n",
    "    <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "</head>\n",
    "<body class=\"bg-light\">\n",
    "<div class=\"container mt-5\">\n",
    "    <h1 class=\"mb-4\">Product Rating Predictor</h1>\n",
    "    <form method=\"post\" action=\"/predict\">\n",
    "        <div class=\"mb-3\">\n",
    "            <label for=\"user_id\" class=\"form-label\">User ID</label>\n",
    "            <input type=\"number\" class=\"form-control\" id=\"user_id\" name=\"user_id\" required>\n",
    "        </div>\n",
    "        <div class=\"mb-3\">\n",
    "            <label for=\"item_id\" class=\"form-label\">Item ID</label>\n",
    "            <input type=\"number\" class=\"form-control\" id=\"item_id\" name=\"item_id\" required>\n",
    "        </div>\n",
    "        <button type=\"submit\" class=\"btn btn-primary\">Predict Rating</button>\n",
    "    </form>\n",
    "    {% if prediction %}\n",
    "    <div class=\"alert alert-success mt-3\">\n",
    "        <strong>Prediction:</strong> {{ prediction }} for User {{ user_id }} and Item {{ item_id }}\n",
    "    </div>\n",
    "    {% endif %}\n",
    "    {% if error %}\n",
    "    <div class=\"alert alert-danger mt-3\">\n",
    "        {{ error }}\n",
    "    </div>\n",
    "    {% endif %}\n",
    "</div>\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4622fc68",
   "metadata": {},
   "source": [
    "# **Conclusion & Next Steps**\n",
    "\n",
    "In this project, I leveraged the Microsoft Azure platform to design and deploy a scalable, production-ready collaborative filtering recommender system for Amazon Electronics data using PySparkâ€™s ALS algorithm.\n",
    "\n",
    "## **Solution Summary**\n",
    "\n",
    "- Data Processing & Filtering: Cleaned and filtered Electronics_5.json to retain relevant users and products.\n",
    "\n",
    "- Model Training & Tuning: Trained a Spark-based ALS model and tuned parameters (rank, regParam), achieving a Test RMSE of 1.2662.\n",
    "\n",
    "- Model Persistence & Serving: Saved the trained model to Azure-mounted persistent storage and created a Flask-based prediction API.\n",
    "\n",
    "- Containerized Deployment: Deployed the entire pipeline (Spark, Flask, Jupyter) within a Docker container on an Azure Virtual Machine.\n",
    "\n",
    "- Interactive Interface: Developed API endpoints and began integrating Bootstrap UI to demonstrate prediction capabilities interactively.\n",
    "\n",
    "## **Fulfillment of Azure Cloud Requirements**\n",
    "\n",
    "### **Requirement\tImplementation**\n",
    "\n",
    "- **Persistent Storage:** Used **Azure File Share mounted to the VM to persist the ALS model under /media/amazonratings/ALS_Model**. This ensures model files are retained across container restarts.\n",
    "\n",
    "- **Compute Resource:**\tDeployed a Dockerized application on an Azure Virtual Machine (VM), acting as the primary compute resource for training, inference, and API serving.\n",
    "\n",
    "- **Network Security:**\tConfigured Network Security Group (NSG) rules to only expose ports 8888 (Jupyter) and 8080 (Flask API). Access is restricted to specific IPs where applicable.\n",
    "\n",
    "- This solution was built entirely using services available under the Azure Free Account, including File Share (via Storage Account), Azure VM (B1s tier), and manual VNet configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bcf027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
