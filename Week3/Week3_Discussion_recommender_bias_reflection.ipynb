{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l80TiyAUojV"
      },
      "source": [
        "# **Reflection: Recommender Systems and Algorithmic Bias**\n",
        "\n",
        "**Name:** Umais Siddiqui\n",
        "\n",
        "**Class:** Data 612 - Recommender Systems\n",
        "\n",
        "**Github Link:** https://github.com/umais/DATA612_Recommender_Systems/blob/master/Week3/Week3_Discussion_recommender_bias_reflection.ipynb\n",
        "\n",
        "## **Introduction**\n",
        "\n",
        "Recommender systems are ubiquitous in today's digital world from recommending what music to listen to, to suggesting products to purchase or news articles to read. As powerful as they are, they are not immune to **algorithmic bias**. In fact, they often **amplify** existing human biases because they learn from historical, human-generated data.\n"
      ],
      "id": "-l80TiyAUojV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQMnGmasUojY"
      },
      "source": [
        "### **How Recommender Systems Reinforce Human Bias**\n",
        "\n",
        "### 1. **Feedback Loops**\n",
        "Once a system starts recommending a certain kind of content (e.g., pop songs, certain political views, fashion styles), users who engage with these recommendations further **strengthen the algorithm’s confidence**, making it harder for diverse or minority content to break through. This is often called the \"**filter bubble**\" or \"**echo chamber**\" effect.\n",
        "\n",
        "*Example:*  \n",
        "A job portal that recommends high-paying tech roles predominantly to men because the training data showed higher application or success rates among men. This can reinforce gender stereotypes in employment sectors. *(Jain, 2016)*\n",
        "\n",
        "### 2. **Biased Training Data**\n",
        "Recommender systems learn patterns from historical data. If this data reflects existing inequalities or prejudices, the system will replicate them. For instance, if fewer women historically bought tech gadgets, a recommender system might recommend them less often to women.\n",
        "\n",
        "*Example:*  \n",
        "A movie recommendation engine might underrepresent films with diverse casts because previous viewing patterns (biased by underpromotion of such films) showed low engagement. *(Estola, 2016)*\n",
        "\n",
        "### 3. **Popularity Bias**\n",
        "Items that are already popular get more exposure, making it hard for niche or minority-interest items to be recommended. This can exclude voices or products from underrepresented communities.\n"
      ],
      "id": "IQMnGmasUojY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqc_s3L6UojZ"
      },
      "source": [
        "## **Do Techniques Help or Hurt?**\n",
        "\n",
        "### **Reinforcement:**\n",
        "Traditional **collaborative filtering** methods especially those using implicit feedback are **susceptible to reinforcing popularity bias and homogeneity**. For example, **ALS (Alternating Least Squares)**, while efficient, only optimizes based on observed preferences and doesn’t correct for exposure bias.\n",
        "\n",
        "### **Prevention:**\n",
        "More recent techniques attempt to address fairness and bias:\n",
        "\n",
        "- **Diversity-aware recommenders** inject constraints to ensure a wider variety of content.\n",
        "- **Fairness-aware machine learning** focuses on equalizing error rates across demographic groups. *(Hardt et al., 2016)*\n",
        "- **Reinforcement Learning**-based recommenders can be optimized for long-term fairness rather than just immediate engagement.\n"
      ],
      "id": "oqc_s3L6UojZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErYfanZeUojZ"
      },
      "source": [
        "## **A Path Forward: Ethical Considerations**\n",
        "\n",
        "To reduce unethical targeting or biased customer segmentation, systems must be:\n",
        "\n",
        "1. **Audited regularly** for disparate impact across groups.\n",
        "2. **Transparent** in how recommendations are made.\n",
        "3. **Designed with fairness constraints** or post-processing techniques to rebalance skewed outputs.\n"
      ],
      "id": "ErYfanZeUojZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVChYMmlUoja"
      },
      "source": [
        "## **References**\n",
        "\n",
        "- Estola, E. (2016). *When Recommendation Systems Go Bad*. MLconf SEA. [https://www.youtube.com/watch?v=7wCDcd5ZQzg](https://www.youtube.com/watch?v=7wCDcd5ZQzg)\n",
        "- Jain, R. (2016). *When Recommendation Systems Go Bad*. Medium. [https://rishabhjain.medium.com/when-recommendation-systems-go-bad-13aeb6f59f8](https://rishabhjain.medium.com/when-recommendation-systems-go-bad-13aeb6f59f8)\n",
        "- Hardt, M., Price, E., & Srebro, N. (2016). *Equality of Opportunity in Supervised Learning*. In *Advances in Neural Information Processing Systems* (NeurIPS). [https://proceedings.neurips.cc/paper_files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf)\n"
      ],
      "id": "VVChYMmlUoja"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}